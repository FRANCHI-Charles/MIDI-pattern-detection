{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02f0ad4e",
   "metadata": {},
   "source": [
    "# First test of PatternLearner network on Fugues Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6320f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import time\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from Fugues_data.loader import FuguesDataset\n",
    "from ML.architecture import PatternLearner, CorrelationLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3cf479a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join(\"Fugues_data\", \"data_8_reduced.pkl\")\n",
    "TEST_SIZE = 0.1\n",
    "VALIDATION_SIZE = 0.2\n",
    "CNN_MODEL_NAME = 'cnn_patterns_'\n",
    "\n",
    "MAX_EPOCH = 200\n",
    "BATCH_SIZE = 10\n",
    "LEARNING_RATE = 0.1\n",
    "EPSILON = 0.0000001\n",
    "PATTERNS_MAXSIZE = (1, 4*8, 13)\n",
    "PATIENCE = 3\n",
    "REFINEMENT = 3  # restart training after patience runs out with the best model, decrease lr by...\n",
    "LR_FAC = 0.1    # ... the learning rate factor lr_fac: lr_new = lr_old*lr_fac\n",
    "LOG_INTERVAL = 60  # seconds\n",
    "\n",
    "MINDIV = 8 #from Fugues_data.midi_to_pkl import MINDIV\n",
    "\n",
    "torch.manual_seed(689)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb3d63f",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ee47fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([210, 1, 3856, 60])\n"
     ]
    }
   ],
   "source": [
    "data = FuguesDataset(DATA_PATH, mindiv=MINDIV)\n",
    "print(data[:].shape)\n",
    "\n",
    "kwargs = {'num_workers': 2, 'pin_memory': True} if torch.cuda.is_available() else {'num_workers': 0}\n",
    "train, validation, test_data = random_split(data, [(1-TEST_SIZE) * (1-VALIDATION_SIZE), (1-TEST_SIZE) * VALIDATION_SIZE, TEST_SIZE])\n",
    "valid_data = next(iter(DataLoader(validation, len(validation), **kwargs))).float().to(device)\n",
    "train_loader = DataLoader(train, BATCH_SIZE, shuffle=True, **kwargs)\n",
    "test_data = next(iter(DataLoader(test_data, len(test_data), **kwargs))).float().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db57c9d2",
   "metadata": {},
   "source": [
    "Each music file is nearly equivalent to a picture of size 1000x500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbc3e60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 3856, 60])\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(train_loader)).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d4dd6b",
   "metadata": {},
   "source": [
    "## Load architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc91610b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after conv 1 : torch.Size([2, 3, 964, 60])\n",
      "Shape after conv 2 : torch.Size([2, 6, 241, 60])\n",
      "Shape after conv 3 : torch.Size([2, 12, 60, 60])\n",
      "Shape after conv 4 : torch.Size([2, 24, 15, 6])\n",
      "Shape after dense Layer : torch.Size([2, 416])\n",
      "torch.Size([2, 1, 32, 13])\n",
      "Number of parameters: 943689\n"
     ]
    }
   ],
   "source": [
    "model = PatternLearner(data[0].shape, PATTERNS_MAXSIZE)\n",
    "\n",
    "test = torch.rand((2, *data[0].shape))\n",
    "print(model(test, True).shape)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044a1183",
   "metadata": {},
   "source": [
    "## Train functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a613f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZER = Adam\n",
    "LOSS_FUNCTION = CorrelationLoss().minmax_regul(beta=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0730bcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_cnn(model, optimizer):\n",
    "    \"\"\"\n",
    "    Training loop for one epoch of NN training.\n",
    "    \"\"\"\n",
    "    model.train()  # set model to training mode (activate dropout layers if any)\n",
    "    t = time() # we measure the needed time\n",
    "    for batch_idx, input_data in enumerate(train_loader):  # iterate over training input_data\n",
    "        input_data = input_data.float().to(device)  # move input_data to device (GPU) if necessary\n",
    "        optimizer.zero_grad()  # reset optimizer\n",
    "        output = model(input_data)   # forward pass: calculate output of network for input_data\n",
    "        loss = LOSS_FUNCTION(output, input_data)\n",
    "\n",
    "        loss.backward()  # backward pass: calculate gradients using automatic diff. and backprop.\n",
    "        optimizer.step()  # udpate parameters of network using our optimizer\n",
    "        cur_time = time()\n",
    "        # print some outputs if we reached our logging interval\n",
    "        if cur_time - t > LOG_INTERVAL or batch_idx == len(train_loader)-1:  \n",
    "            print(f\"[{batch_idx * BATCH_SIZE}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\",\n",
    "                  f\"\\tloss: {loss.item():.6f}, took {cur_time - t:.2f}s\")\n",
    "            t = cur_time\n",
    "\n",
    "\n",
    "def valid_cnn(model):\n",
    "    \"\"\"\n",
    "    Test loss evaluation\n",
    "    \"\"\"\n",
    "    model.eval()  # set model to inference mode (deactivate dropout layers)\n",
    "    with torch.no_grad():  # do not calculate gradients since we do not want to do updates\n",
    "        output = model(test_data)\n",
    "        loss = LOSS_FUNCTION(output, test_data)\n",
    "    print(f'Average eval loss: {loss:.4f}\\n')\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9cb339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cnn():\n",
    "    \"\"\"\n",
    "    Run CNN training using the datasets.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "        nn.Model\n",
    "            trained model\n",
    "    \"\"\"\n",
    "\n",
    "    # create model and optimizer, we use plain SGD with momentum\n",
    "    optimizer = OPTIMIZER(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    model_cnt = 0\n",
    "    new_model_file = os.path.join(CNN_MODEL_NAME + str(model_cnt) + '.model')\n",
    "    while os.path.exists(new_model_file):\n",
    "        model_cnt += 1\n",
    "        new_model_file = os.path.join(CNN_MODEL_NAME + str(model_cnt) + '.model')\n",
    "    \n",
    "\n",
    "    # train model for max_epochs epochs, output loss after log_intervall seconds.\n",
    "    # for each epoch run once on validation set, \n",
    "    # write model to disk if validation loss decreased\n",
    "    # if validation loss increased, check for early stopping with patience and refinements\n",
    "    # after model is trained, perform a run on test set and output loss (don't forget to reload best model!)\n",
    "    best_valid_loss = 9999.\n",
    "    cur_patience = PATIENCE\n",
    "    cur_refin = REFINEMENT\n",
    "\n",
    "    #model.load_state_dict(torch.load(last_model_file, map_location=device).state_dict())\n",
    "    print('Training CNN...')\n",
    "    start_t = time()\n",
    "\n",
    "    for epoch in range(1, MAX_EPOCH+1):\n",
    "        train_epoch_cnn(model, optimizer)\n",
    "        valid_loss = valid_cnn(model)\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            torch.save(model, new_model_file)\n",
    "            best_valid_loss = valid_loss\n",
    "            cur_patience = PATIENCE\n",
    "\n",
    "        elif cur_patience <=0:\n",
    "            model.load_state_dict(torch.load(new_model_file, map_location=device).state_dict())\n",
    "            if cur_refin <= 0:\n",
    "                print(\"Max refinement reached !\")\n",
    "                break\n",
    "            else:\n",
    "                print(\"Max patience reached !\")\n",
    "                \n",
    "                cur_patience = PATIENCE\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    lr = LR_FAC * param_group['lr']\n",
    "                    param_group['lr'] = lr\n",
    "                cur_refin -= 1\n",
    "            \n",
    "        else:\n",
    "            print(\"We still have patience...\")\n",
    "            cur_patience -= 1\n",
    "    \n",
    "    print(f'Training took: {time()-start_t:.2f}s for {epoch} epochs')\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def load_cnn(load_model:str):\n",
    "    \"Load the model.\"\n",
    "    if load_model is None or not os.path.exists(load_model):\n",
    "        print('Model file not found, unable to load...')\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(load_model, map_location=device).state_dict())\n",
    "        print(\"Model file loaded: {}\".format(load_model))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9ba0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cnn()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IS2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
